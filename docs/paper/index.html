<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Reproducible Methods for Face Research</title>

<script src="index_files/header-attrs-2.7/header-attrs.js"></script>
<script src="index_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="index_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="index_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<link href="index_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="index_files/highlightjs-9.12.0/highlight.js"></script>
<script src="index_files/kePrint-0.0.1/kePrint.js"></script>
<link href="index_files/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>


<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>


<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-sm-12 col-md-4 col-lg-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-sm-12 col-md-8 col-lg-9">




<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Reproducible Methods for Face Research</h1>
<h4 class="author">Lisa DeBruine & Iris Holzleitner (DRAFT)</h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>Face stimuli are commonly created in non-reproducible ways. This paper will introduce the open-access online platform webmorph and its associated R package webmorphR. It will explain the technical processes of morphing and transforming through a case study of creating face stimuli from an open-access image set.</p>
</div>

</div>


<style>
  blockquote { 
    font-size: 13px; 
    border: 1px solid grey; 
    background-color: #EEE;
    border-radius: 1em;
  }
</style>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Face stimuli are commonly used in research on visual and social perception. This almost always involves some level of stimulus preparation to rotate, resize, crop, and reposition faces on the image. In addition, many studies systematically manipulate face images by changing color and/or shape properties <span class="citation">(e.g., Alex L. Jones and Jaeger 2019; reviewed in Little, Jones, and DeBruine 2011)</span>.</p>
<p>Gronenschild and colleagues <span class="citation">(2009)</span> argue for the importance of standardizing face stimuli so that they are not “confounded by factors such as brightness and contrast, head size, hair cut and color, skin color, and the presence of glasses and earrings.” They describe a three-step standardization process. First, they manually removed features such as glasses and earrings in Photoshop. Second, they geometrically standardized images by semi-automatically defining eye and mouth coordinates used to fit the images within an oval mask, Third, they optically standardized images by converting them to greyscale and remapping values between the minimum and 98% threshold onto the full range of values. While laudable in its aims, this procedure has not achieved widespread adoption, probably because the authors provided no code or tools. In personal communication, the main author said that this is because “the procedure is based on standard image processing algorithms described in many textbooks.” However, I was unable to easily replicate the procedure and found several places where instructions had more than one possible interpretation or relied on the starting images having specific properties, such as symmetric lighting reflections in the eyes.</p>
<p>Scope of this type of research. How many papers, what range of questions?</p>
<div id="reproducibility" class="section level3">
<h3>Reproducibility!</h3>
<p>Why are reproducible stimulus construction methods important?</p>
<p>I once gave up on a research project because I couldn’t figure out how to manipulate spatial frequency in MatLab to make my stimuli look like those in a relevant paper. When I contacted the author, they didn’t know how the stimuli were created because a postdoc had done it in Photoshop and didn’t leave a detailed record of the method.</p>
<p>Reproducibility of stimuli is especially important for face stimuli because faces are sampled, so replications should sample new <em>faces</em> as well as new participants <span class="citation">(Barr 2007)</span>. The difficulty of creating equivalent face stimuli is a major barrier to this, resulting in stimulus sets that are used across dozens or hundreds of papers. For example, the Chicago Face Database <span class="citation">(<span>“The Chicago Face Database: A Free Stimulus Set of Faces and Norming Data.”</span> 2015)</span> has been cited in almost 800 papers. Ekman and Friesen’s <span class="citation">(1976)</span> Pictures of Facial Affect has been cited more than 5500 times. This image set is currently <a href="https://www.paulekman.com/product/pictures-of-facial-affect-pofa/"><strong>selling</strong> for $399</a> for “110 photographs of facial expressions that have been widely used in cross-cultural studies, and more recently, in neuropsychological research.” Such extensive reuse of image sets means that any confounds present in the image set can cause highly “replicable” but false findings.</p>
<p>Additionally, image sets are often private and reused without clear attribution. The Face research Lab has only recently been trying to combat this by making image sets public and citable where possible <span class="citation">(e.g., L. M. DeBruine and Jones 2017b, 2017a; Morrison et al. 2018)</span> and including clear explanations of reuse where not possible <span class="citation">Holzleitner et al. (2019)</span>.</p>
</div>
<div id="common-techniques" class="section level3">
<h3>Common Techniques</h3>
<p>It was basically impossible to systematically survey the literature about the methods used to create facial stimuli, in large part because of poor documentation. However, several common methods are discussed below.</p>
<div id="photoshopimage-editors" class="section level4">
<h4>Photoshop/Image editors</h4>
<p>A search for “Photoshop face attractiveness” produced 6,450 responses in Google Scholar. Here are descriptions of the use of Photoshop from a few of the top hits.</p>
<blockquote>
<p>If necessary, scanned pictures were rotated slightly, using Adobe Photoshop software, clockwise to counterclockwise until both pupil centres were on the same y-coordinate. Each picture was slightly lightened a constant amount by Adobe Photoshop. <span class="citation">(Scheib, Gangestad, and Thornhill 1999, 1914)</span></p>
</blockquote>
<blockquote>
<p>For each face in the adapting set, we created two distortions using the spherize distort function in Adobe Photoshop: a −50% distortion, which compressed the center of the face, producing the appearance of a face in a concave mirror, and a +50% distortion, which expanded the center of the face, producing the appearance of a face in a convex mirror <span class="citation">(Rhodes et al. 2003, 559)</span></p>
</blockquote>
<blockquote>
<p>Using Adobe Photoshop, Version 3.05 (1994), each photo was rotated (if necessary) and then cut vertically along the facial midline using the philtrum (the base of the nose) as a guide. For each LL image, a copy of the left half of the face was reversed horizontally and then pasted onto the actual left half of the photograph (as depicted in Figure 1). The same procedure was followed to obtain RR mirror-image depictions. <span class="citation">(Mealey, Bridgstock, and Townsend 1999, 153)</span></p>
</blockquote>
<blockquote>
<p>Averted gaze images were generated in Adobe Photoshop. The irises and pupils of each face were isolated using the path tool and horizontally shifted three pixels left and right to produce an averted-left and averted-right version of each face <span class="citation">(Ewing, Rhodes, and Pellicano 2010, 324)</span></p>
</blockquote>
<blockquote>
<p>These pictures were edited using Adobe Photoshop 6.0 to remove external features (hair, ears) and create a uniform grey background. <span class="citation">(Sforza et al. 2010, 150)</span></p>
</blockquote>
</div>
<div id="commerical-morphing" class="section level4">
<h4>Commerical morphing</h4>
<p>Face averaging or “morphing” is a common technique for making images that are blends of two or more faces. We found 831 Google Scholar responses for “Fantamorph face,” 158 Google Scholar responses for “WinMorph face” and fewer mentions of several other programs, such as MorphThing (no longer available) and xmorph.</p>
<p>Most of these programs do not use open formats for storing delineations, the x- and y-coordinates of the landmark points that define shape and the way these are connected with lines. Their algorithms also tend to be closed and there is no common language for describing the procedures used to create stimuli in one program in a way that is easily translatable to another program. Here are descriptions of the use of commercial morphing programs from a few of the top hits.</p>
<blockquote>
<p>The faces were carefully marked with 112 nodes in FantaMorph™, 4th version: 28 nodes (face outline), 16 (nose), 5 (each ear), 20 (lips), 11 (each eye), and 8 (each eyebrow). To create the prototypes, I used FantaMorph Face Mixer, which averages node locations across faces. Prototypes are available online, in the Personality Faceaurus [<a href="http://www.nickholtzman.com/faceaurus.htm" class="uri">http://www.nickholtzman.com/faceaurus.htm</a>]. <span class="citation">(Holtzman 2011, 650)</span></p>
</blockquote>
<p>The link above contains only morphed face images and no further details about the morphing or stimulus preparation procedure.</p>
<blockquote>
<p>For the stimuli used as probes of the aftereffect, we created morphs across the three expression pairs for each Karolinska face, using Fantamorph 3.0 (www.fantamorph.com). Twenty-one images were produced for each morph series, with each picture representing a 5% step within the morph series (i.e., 0/100, 5/95, 10/90… 100/0). <span class="citation">(Fox and Barton 2007, 87)</span></p>
</blockquote>
<blockquote>
<p>The 20 individual stimuli of each category were paired to make 10 morph continua, by morphing one endpoint exemplar into its paired exemplar (e.g. one face into its paired face, see Figure 1C) in steps of 5%. Morphing was realized within FantaMorph Software (Abrosoft) for faces and cars, Poser 6 for bodies (only between stimuli of the same gender with same clothing), and Google SketchUp for places. <span class="citation">(Weigelt, Koldewyn, and Kanwisher 2013, 4)</span></p>
</blockquote>
<blockquote>
<p>Each identity was morphed, using Fantamorph v5.3.1 (<a href="http://www" class="uri">http://www</a>. fantamorph.com), with three composite faces displaying happy, angry, and neutral expressions, respectively (each an average of 50 identities, from Skinner &amp; Benton, 2010). For the neutral face condition, we also morphed each original face (neutral expression) image towards a neutral composite. This process ensured that all stimuli presented during this task were morphs. Standard morphing procedures were used to create 25% and 50% morphs by blending the original faces with the expression composites in different proportions, for example, 25% angry morph was a 75/25 blend of an original face and the angry composite. <span class="citation">(Caulfield et al. 2016, 506–7)</span></p>
</blockquote>
<blockquote>
<p>Then, for each pair, different degrees of digital morphing between the two picture faces were created using Abrosoft Fantamorph 3.0 (from 0% to 100% in steps of 2%, for a total of 48 morphs, plus the two original pictures; Figure 2A). Along the morphing continuum, 2% steps were contemplated except for two specific points where the step was 3% (from 24% to 27% and from 73% to 76%). <span class="citation">(Sforza et al. 2010, 150–51)</span></p>
</blockquote>
</div>
<div id="scriptable-methods" class="section level4">
<h4>Scriptable Methods</h4>
<p>There are several scriptable methods for creating image stimuli, including MatLab, ImageMagick, and GraphicConvertor.</p>
<p>MatLab <span class="citation">(Higham and Higham 2016)</span> is widely used within visual psychophysics. A Google Scholar search for “MatLab face attractiveness” returned 7,440 hits, although the majority of papers I inspected used MatLab to process EEG data, present the experiment, or analyse image color, rather than using MatLab to create the stimuli. “MatLab face perception” generated 80,800 hits, more of which used MatLab to create stimuli.</p>
<blockquote>
<p>Subjects were presented with four different types of face stimuli: black and white line drawings of unfamiliar faces, and gray scale photographs of unfamiliar, famous, and emotional faces. Phase scrambled versions of these faces were used as visual baseline. The scrambled pictures were generated by randomizing the phase information after Fourier transformation using an in-house Matlab script. <span class="citation">(no stimuli were shown in the paper, Ishai, Schmidt, and Boesiger 2005, 88)</span></p>
</blockquote>
<blockquote>
<p>The average pixel intensity of each image (ranging from 0 to 255) was set to 128 with a standard deviation of 40 using the SHINE toolbox (function lumMatch) (Willenbockel et al., 2010) in MATLAB (version 8.1.0.604, R2013a). <span class="citation">(Visconti di Oleggio Castello et al. 2014, 2)</span></p>
</blockquote>
<p>ImageMagick <span class="citation">(The ImageMagick Development Team 2021)</span> is a free, open-source program that creates, edits, and converts images in a scriptable manner. The {magick} R package allows you to script image manipulations in R using ImageMagick <span class="citation">(Ooms 2021)</span>.</p>
<blockquote>
<p>Images were cropped, resized to 150 × 150 pixels, and then grayscaled using ImageMagick (version 6.8.7-7 Q16, x86_64, 2013-11-27) on Mac OS X 10.9.2. <span class="citation">(Visconti di Oleggio Castello et al. 2014, 2)</span></p>
</blockquote>
<p>GraphicConvertor <span class="citation">(Nishimura 2000)</span> is typically used to batch process images, such as making images a standard size or adjusting color. While not technically “scriptable,” batch processing can be set up in the GUI interface and the saved to a reloadable “.gaction” file. (A search for ‘“gaction” graphicconvertor’ on Google Scholar returned no hits.)</p>
<blockquote>
<p>Photographs were converted to black-and-white images through a program called GraphicConverter (Thorsten Lemke, v.3.1.1). <span class="citation">(Neiworth, Hassett, and Sylvester 2007, 128–29)</span></p>
</blockquote>
<blockquote>
<p>We used the GraphicConverterTM application to crop the images around the cat face and make them all 1024x1024 pixels. One of the challenges of image matching is to do this process automatically. <span class="citation">(Paluszek and Thomas 2019, 214)</span></p>
</blockquote>
</div>
<div id="mystery-methods" class="section level4">
<h4>Mystery Methods</h4>
<p>Many researchers describe image manipulation generically or use “in-house” methods that are not well specified enough for another researcher to have any chance of replicating them.</p>
<blockquote>
<p>A normalization procedure was then used to bring each face into a common orientation and position. <span class="citation">(Pantelis et al. 2008, 1188)</span></p>
</blockquote>
<blockquote>
<p>Each of the images was rendered in gray-scale and morphed to a common shape using an in-house program based on bi-linear interpolation (see e.g., Gonzalez &amp; Woods, 2002). Key points in the morphing grid were set manually, using a graphics program to align a standard grid to a set of facial points (eye corners, face outline, etc.). Images were then subject to automatic histogram equalization. <span class="citation">(Burton et al. 2005, 263)</span></p>
</blockquote>
<p>The reference above <span class="citation">(Gonzalez, Woods, and others 2002)</span> has been cited by 2384 papers on Google Scholar, and is a 190-page book. It mentions bilinear interpolation on pages 64–66 in the context of calculating pixel color when resizing images and it’s unclear how this could be used to morph shape.</p>
<blockquote>
<p>Each face was put into an oval so that the hairstyle was not visible. The oval was 670 pixels high x 510 pixels wide, which corresponded to a size of about 14.5 x 11 cm on the screen. [baudouin2006face, p. 792]</p>
</blockquote>
</div>
<div id="psychomorphwebmorph" class="section level4">
<h4>Psychomorph/WebMorph</h4>
<p><span class="citation">(Benson and Perrett 1993)</span>
<span class="citation">(Rowland and Perrett 1995)</span>
<span class="citation">(L. M. DeBruine 2018)</span></p>
</div>
</div>
<div id="ethical-issues" class="section level3">
<h3>Ethical Issues</h3>
<p>Research with identifiable faces has a number of ethical issues. This means it is not always possible to share the exact images used in a study. Then it is all the more important for the stimulus construction methods to be clear and reproducible. However, there are other ethical issues outside of image sharing that we feel are important to highlight in an paper discussing the use of face images in research.</p>
<p>The use of face photographs must respect participant consent and personal data privacy. Images that are “freely” available on the internet are a grey area and the ethical issues should be carefully considered and approved by the relevant ethics board.</p>
<p>Do not use face images in research where there is a possibility of real-world consequences for the pictured individuals. For example, do not post identifiable images of real people on real dating sites without the explicit consent of the pictured individuals for that specific research.</p>
<p>The use of face image analysis should never be used to predict behaviour or as automatic screening. For example, face images cannot be used to predict criminality or decide who should proceed to the interview stage in a job application. This type of application is unethical because the training data is always biased. Face image analysis can be useful for researching what aspects of face images give rise to the <em>perception</em> of traits like trustworthiness, but should not be confused with the ability to detect <em>actual</em> behaviour. Researchers have a responsibility to consider how their research may be misused in this manner.</p>
</div>
<div id="glossary" class="section level3">
<h3>Glossary</h3>
<table class="table table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:glossary">Table 1: </span>Glossary of terms.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Term
</th>
<th style="text-align:left;">
Definition
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
delineation
</td>
<td style="text-align:left;">
the x- and y-coordinates for a specific template that describe an image
</td>
</tr>
<tr>
<td style="text-align:left;">
landmark
</td>
<td style="text-align:left;">
a point that marks corresponding locations on different images
</td>
</tr>
<tr>
<td style="text-align:left;">
lines
</td>
<td style="text-align:left;">
connections between landmarks; these may be used to interpolate new landmarks for morphing
</td>
</tr>
<tr>
<td style="text-align:left;">
morphing
</td>
<td style="text-align:left;">
blending two or more images to make an image with an average shape andor color
</td>
</tr>
<tr>
<td style="text-align:left;">
prototype
</td>
<td style="text-align:left;">
an average of faces with similar characteristics, such as expression, gender, age, and/or ethnic group
</td>
</tr>
<tr>
<td style="text-align:left;">
template
</td>
<td style="text-align:left;">
a set of landmark points that define shape and the way these are connected with lines; only image with the same template can be averaged or transformed
</td>
</tr>
<tr>
<td style="text-align:left;">
transforming
</td>
<td style="text-align:left;">
changing the shape and/or color of an image by some proportion of a vector that is defined as the difference between two images
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="methods" class="section level2">
<h2>Methods</h2>
<div id="editing" class="section level3">
<h3>Editing</h3>
<p>Almost all imagesets start with raw images that need to be cropped, resized, rotated, padded, and/or color normalised Although many reproducible methods exist to manipulate images in these ways, they are complicated when an image has an associated template, so {webmorphR} has functions that alter the image and template together (Fig. <a href="#fig:editing">1</a>).</p>
<pre class="r"><code>orig &lt;- demo_stim(&quot;test&quot;, 1) # load one demo image
mirrored &lt;- mirror(orig)
cropped &lt;- crop(orig, width = 0.75, height = 0.75)
resized &lt;- resize(orig, 0.75)
rotated &lt;- rotate(orig, degrees = 180)
padded &lt;- pad(orig, 30, fill = &quot;black&quot;)
grey &lt;- greyscale(orig)</code></pre>
<div class="figure"><span id="fig:editing"></span>
<img src="index_files/figure-html/editing-1.png" alt="Examples of image manipulations: (A) original image, (B) mirrored, (C) cropped to 75%, (D) resized to 75%, (E) rotated 180 degrees, (F) 30 pixels of black padding added, and (G) greyscale,." width="100%" />
<p class="caption">
Figure 1: Examples of image manipulations: (A) original image, (B) mirrored, (C) cropped to 75%, (D) resized to 75%, (E) rotated 180 degrees, (F) 30 pixels of black padding added, and (G) greyscale,.
</p>
</div>
</div>
<div id="delineation" class="section level3">
<h3>Delineation</h3>
<p>The image manipulations above work best if your raw images start the same size and aspect ratio, with the faces in the same orientation and position on each image. This is frequently not the case with raw images. Image delineation provides a way to set image manipulation parameters relative to face landmarks by marking corresponding points according to a template.</p>
<p>WebMorph’s default face template marks 189 points (Fig. <a href="#fig:delineate">2</a>). Some of these points have very clear anatomical locations, such as point 0 (“left pupil”), while others have only approximate placements and are used mainly for masking or preventing morphing artifacts from affecting the background of images, such as point 147 (“about 2cm to the left of the top of the left ear (creates oval around head)”). Template points numbering is 0-based because PsychoMorph was originally written in Java.</p>
<div class="figure"><span id="fig:delineate"></span>
<img src="index_files/figure-html/delineate-1.png" alt="Default webmorph FRL template" width="100%" />
<p class="caption">
Figure 2: Default webmorph FRL template
</p>
</div>
<p>The function <code>tem_def()</code> retrieves a template definition that includes point names, default coordinates, and the identity of the symmetrically matching point for mirroring or symmetrising images Table <a href="#tab:tem-def">2</a>.</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:tem-def">Table 2: </span>The first 10 landmark points of WebMorph’s default “FRL” template.
</caption>
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:left;">
name
</th>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:right;">
sym
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
left pupil
</td>
<td style="text-align:right;">
165.659
</td>
<td style="text-align:right;">
275.234
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
right pupil
</td>
<td style="text-align:right;">
284.339
</td>
<td style="text-align:right;">
275.231
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
top of left iris
</td>
<td style="text-align:right;">
165.219
</td>
<td style="text-align:right;">
267.225
</td>
<td style="text-align:right;">
10
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
top-left of left iris
</td>
<td style="text-align:right;">
155.781
</td>
<td style="text-align:right;">
269.909
</td>
<td style="text-align:right;">
17
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
left of left iris
</td>
<td style="text-align:right;">
154.252
</td>
<td style="text-align:right;">
276.694
</td>
<td style="text-align:right;">
16
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
bottom-left of left iris
</td>
<td style="text-align:right;">
157.117
</td>
<td style="text-align:right;">
282.607
</td>
<td style="text-align:right;">
15
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
bottom of left iris
</td>
<td style="text-align:right;">
165.507
</td>
<td style="text-align:right;">
286.083
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
bottom-right of left iris
</td>
<td style="text-align:right;">
173.839
</td>
<td style="text-align:right;">
282.517
</td>
<td style="text-align:right;">
13
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
right of left iris
</td>
<td style="text-align:right;">
176.720
</td>
<td style="text-align:right;">
276.472
</td>
<td style="text-align:right;">
12
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
top-right of left iris
</td>
<td style="text-align:right;">
175.040
</td>
<td style="text-align:right;">
270.366
</td>
<td style="text-align:right;">
11
</td>
</tr>
</tbody>
</table>
<p>You can automatically delineate faces with a simpler template (Fig. <a href="#fig:auto-delin">3</a>) using services provided through <a href="https://www.faceplusplus.com/landmarks/">Face++</a>.</p>
<pre class="r"><code># load 5 images with FRL templates
f &lt;- demo_stim(&quot;london&quot;, &quot;006|038|064|066|135&quot;)

# remove templates and auto-delineate with Face++ template
fpp_tem &lt;- auto_delin(f, replace = TRUE)</code></pre>
<div class="figure"><span id="fig:auto-delin"></span>
<img src="index_files/figure-html/auto-delin-1.png" alt="Delineation templates: (A) manual delineations using the FRL template and (B) automatic delineations using the Face++ template." width="100%" />
<p class="caption">
Figure 3: Delineation templates: (A) manual delineations using the FRL template and (B) automatic delineations using the Face++ template.
</p>
</div>
<p>A study comparing the accuracy of four common measures of face shape (sexual dimorphism, distinctiveness, bilateral asymmetry, and facial width to height ratio) between automatic and manual delineation concluded that automatic delineation had higher replicability and good correlations with manual delineation <span class="citation">(Alex L. Jones, Schild, and Jones 2020)</span>. However, &lt;2% of images had noticeably inaccurate automatic delineation, which should be screened for by outlier detection and visual inspection.</p>
<p>While automatic delineation has the advantage of being very fast and generally more replicable than manual delineation, it is more limited in the areas that can be described. Typically, automatic face detection algorithms outline the lower face shape and internal features of the face, but don’t define the hairline, hair, neck, or ears. Manual delineation of these can greatly improve stimuli created through morphing or transforming (Fig. <a href="#fig:avg-comp">4</a>).</p>
<div class="figure"><span id="fig:avg-comp"></span>
<img src="index_files/figure-html/avg-comp-1.png" alt="Averages of 5 images made using (A) the full 189-point manual template and (B) the reduced 106-point automatic template." width="100%" />
<p class="caption">
Figure 4: Averages of 5 images made using (A) the full 189-point manual template and (B) the reduced 106-point automatic template.
</p>
</div>
</div>
<div id="facial-metrics" class="section level3">
<h3>Facial Metrics</h3>
<p>Once you have images delineated, you can use the x- and y-coordinates to calculate various facial-metric measurements (Table <a href="#tab:metrics">3</a>). You can calculate the distance between any two points, such as the pupil centres, or use a more complicated formula, such as the face width-to-height ratio from Lefevre et al. <span class="citation">(2013)</span>.</p>
<pre class="r"><code># inter-pupillary distance between points 0 and 1
ipd &lt;- metrics(f, c(0, 1))

# face width-to-height ratio
# width = maximum bizygomatic width
# height = top of upper lip to highest eyelid point
fwh &lt;- metrics(f, &quot;abs(max(x[113],x[112],x[114])-min(x[110],x[111],x[109]))/abs(y[90]-min(y[20],y[25]))&quot;)</code></pre>
<table class="table table-striped table-responsive" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:metrics">Table 3: </span>Facial metric measurements.
</caption>
<thead>
<tr>
<th style="text-align:left;">
face
</th>
<th style="text-align:right;">
x0
</th>
<th style="text-align:right;">
y0
</th>
<th style="text-align:right;">
x1
</th>
<th style="text-align:right;">
y1
</th>
<th style="text-align:right;">
ipd
</th>
<th style="text-align:right;">
fwh
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
006_03
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
620
</td>
<td style="text-align:right;">
776
</td>
<td style="text-align:right;">
630
</td>
<td style="text-align:right;">
206.2426
</td>
<td style="text-align:right;">
2.218905
</td>
</tr>
<tr>
<td style="text-align:left;">
038_03
</td>
<td style="text-align:right;">
580
</td>
<td style="text-align:right;">
580
</td>
<td style="text-align:right;">
793
</td>
<td style="text-align:right;">
577
</td>
<td style="text-align:right;">
213.0211
</td>
<td style="text-align:right;">
2.636580
</td>
</tr>
<tr>
<td style="text-align:left;">
064_03
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
578
</td>
<td style="text-align:right;">
783
</td>
<td style="text-align:right;">
570
</td>
<td style="text-align:right;">
213.1502
</td>
<td style="text-align:right;">
2.351220
</td>
</tr>
<tr>
<td style="text-align:left;">
066_03
</td>
<td style="text-align:right;">
562
</td>
<td style="text-align:right;">
595
</td>
<td style="text-align:right;">
790
</td>
<td style="text-align:right;">
599
</td>
<td style="text-align:right;">
228.0351
</td>
<td style="text-align:right;">
2.281818
</td>
</tr>
<tr>
<td style="text-align:left;">
135_03
</td>
<td style="text-align:right;">
573
</td>
<td style="text-align:right;">
639
</td>
<td style="text-align:right;">
788
</td>
<td style="text-align:right;">
639
</td>
<td style="text-align:right;">
215.0000
</td>
<td style="text-align:right;">
2.280788
</td>
</tr>
</tbody>
</table>
<p>While it is <em>possible</em> to calculate metrics such as width-to-height ratio from 2D face images, this does not mean it is a good idea. Even on highly standardized images, head tilt can have large effects on such measurements. When image qualities such as camera type and head-to-camera distance are not standardized, facial metrics are meaningless at best <span class="citation">(Třebickỳ et al. 2016)</span>.</p>
</div>
<div id="alignment" class="section level3">
<h3>Alignment</h3>
<p>If your image set isn’t highly standardised, you probably want to crop, resize and rotate your images to get them all in approximately the same orientation on images of the same size. There are several reproducible options, each with pros and cons.</p>
<p>One-point alignment (Fig. <a href="#fig:norm-comp">5</a>A) doesn’t rotate or resize the image at all, but aligns one of the delineation points across images. This is ideal when you know that your camera-to-head distance and orientation was standard (or meaningfully different) across images and you want to preserve this in the stimuli, but you still need to get them all in the same position and image size.</p>
<p>Two-point alignment (Fig. <a href="#fig:norm-comp">5</a>B) resizes and rotates the images so that two points (usually the centres of the eyes) are in the same position on each image. This will alter relative head size such that people with very close-set eyes will appear to have larger heads than people with very wide-set eyes. This technique is good for getting images into the same orientation when you didn’t have any control over image rotation and camera-to-head distance of the original photos.</p>
<p>Procrustes alignment (Fig. <a href="#fig:norm-comp">5</a>C) resizes and rotates the images so that each delineation point is as aligned as possible across all images. This can obscure meaningful differences in relative face size (e.g., a baby’s face will be as large as an adult’s), but can be superior to two-point alignment. While this requires that the whole face be delineated, you can use a minimal template such as a face outline or the Face++ auto-delineation to achieve good results. While not available in webmorph.org, procrustes alignment is available in webmorphR.</p>
<p>You can very quickly delineate an image set with a custom template using the <code>quick_delin()</code> function in webmorphR if auto-delineation doesn’t provide suitable points.</p>
<pre class="r"><code># one-point alignment
onept &lt;- align(f, pt1 = 55, pt2 = 55,
               x1 = width(f)/2, y1 = height(f)/2,
               fill = &quot;dodgerblue&quot;)

# two-point alignment
twopt &lt;- align(f, pt1 = 0, pt2 = 1, fill = &quot;dodgerblue&quot;)

# procrustes alignment
proc &lt;- align(f, pt1 = 0, pt2 = 1, procrustes = TRUE, fill = &quot;dodgerblue&quot;)</code></pre>
<div class="figure"><span id="fig:norm-comp"></span>
<img src="index_files/figure-html/norm-comp-1.png" alt="Original images with different alignments. (A) One-point alignment placing the bottom of the nose point in the centre of the image. (B) Two-point alignment placing the eye centre points in the same position as the average image. (C) Procrustes alignment moved, rotated, and resized all images to most closely match the average face. A blue background was used to highlight the difference here, but normally a colour matching the image background would be used or the images would be cropped." width="100%" />
<p class="caption">
Figure 5: Original images with different alignments. (A) One-point alignment placing the bottom of the nose point in the centre of the image. (B) Two-point alignment placing the eye centre points in the same position as the average image. (C) Procrustes alignment moved, rotated, and resized all images to most closely match the average face. A blue background was used to highlight the difference here, but normally a colour matching the image background would be used or the images would be cropped.
</p>
</div>
</div>
<div id="masking" class="section level3">
<h3>Masking</h3>
<p>(effect in masc paper) <span class="citation">(L. M. DeBruine et al. 2006)</span>.</p>
<p>The “standard oval mask” has enjoyed widespread popularity because it is straightforward to add to images using programs like PhotoShop. WebmorphR’s <code>mask_oval()</code> function allows you to set oval boundaries manually (Fig. <a href="#fig:mask">6</a>A) or in relation to minimum and maximum template coordinates for each face (Fig. <a href="#fig:mask">6</a>B). An arguably better way to mask out hair, clothing and background from images is to crop around the curves defined by the template (Fig. <a href="#fig:mask">6</a>C).</p>
<pre class="r"><code># standard oval mask
bounds &lt;- list(t = 200, r = 400, b = 300, l = 400)
oval &lt;- mask_oval(f, bounds, fill = &quot;dodgerblue&quot;)

# template-aware oval mask
oval_tem &lt;- f %&gt;%
  subset_tem(features(&quot;gmm&quot;)) %&gt;% # remove external points
  mask_oval(fill = &quot;dodgerblue&quot;) # oval boundaries to max and min template points

# template aware mask
masked &lt;- mask(f, c(&quot;face&quot;, &quot;neck&quot;, &quot;ears&quot;), fill = &quot;dodgerblue&quot;)</code></pre>
<div class="figure"><span id="fig:mask"></span>
<img src="index_files/figure-html/mask-1.png" alt="Images masked with (A) an oval defined by image coordinates, (B) an oval defined by the minimum and maximum x- and y-coordinates of template points, or (C) to include face, ears and neck." width="100%" />
<p class="caption">
Figure 6: Images masked with (A) an oval defined by image coordinates, (B) an oval defined by the minimum and maximum x- and y-coordinates of template points, or (C) to include face, ears and neck.
</p>
</div>
</div>
<div id="averaging" class="section level3">
<h3>Averaging</h3>
<p>Creating average images (also called composite or prototype images) through morphing can be a way to visualise the differences between groups <span class="citation">(Burton et al. 2005)</span>, manipulate averageness <span class="citation">(Little, Jones, and DeBruine 2011)</span>, or create prototypical faces for image transformations.</p>
<p>Averaging faces with texture <span class="citation">(Tiddeman, Burt, and Perrett 2001; Tiddeman, Stirrat, and Perrett 2005)</span> makes composite image look more realistic (Fig. <a href="#fig:avg-texture">7</a>A). However, averages created without texture averaging look smoother and may be more appropriate for transforming color (Fig. <a href="#fig:avg-texture">7</a>B).</p>
<pre class="r"><code>avg_tex &lt;- avg(f, texture = TRUE)
avg_notex &lt;- avg(f, texture = FALSE)</code></pre>
<div class="figure"><span id="fig:avg-texture"></span>
<img src="index_files/figure-html/avg-texture-1.png" alt="An average of 5 faces created (A) with texture averaging and (B) without." width="100%" />
<p class="caption">
Figure 7: An average of 5 faces created (A) with texture averaging and (B) without.
</p>
</div>
</div>
<div id="transforming" class="section level3">
<h3>Transforming</h3>
<p>Transforming alters the appearance of one face by some proportion of the differences between two other faces. This technique is distinct from morphing. For example, you can transform a face in the dimension of sexual dimorphism by calculating the shape and color differences between a prototype female face (Fig. <a href="#fig:trans-vs-morph">8</a>A) and a prototype male face (Fig. <a href="#fig:trans-vs-morph">8</a>B). If you morph an individual female face with these images, you get faces that are halfway between the indivual and prototype faces (Fig. <a href="#fig:trans-vs-morph">8</a>C,D). If you transform the individual face by 50% of the prototype differences, you get feminised and masculinized versions of the individual face (Fig. <a href="#fig:trans-vs-morph">8</a>E,F).</p>
<div class="figure"><span id="fig:trans-vs-morph"></span>
<img src="index_files/figure-html/trans-vs-morph-1.png" alt="Morphing versus transforming: (A, B) composite images, (C, D) averages of the composites with an individual image, (E, F) transforms of an individual image along the male-female continuum." width="100%" />
<p class="caption">
Figure 8: Morphing versus transforming: (A, B) composite images, (C, D) averages of the composites with an individual image, (E, F) transforms of an individual image along the male-female continuum.
</p>
</div>
<p>If, for example, the individual female face was more feminine than the average female face, morphing with the average female face produces an image that is <em>less</em> feminine than the original individual, while transforming along the male-female dimension produces and image that is always <em>more</em> feminine than the original. Morphing with a prototype also results in an image with increased averageness, while transforming maintains individually distinctive features.</p>
<p>Transforming also allows you to manipulate shape and colour independently (Fig. <a href="#fig:trans-shape-color">9</a>).</p>
<div class="figure"><span id="fig:trans-shape-color"></span>
<img src="index_files/figure-html/trans-shape-color-1.png" alt="Transforming shape and color independently: (A) original individual image, (B) shape only, (C), color only, (D) both shape and color." width="100%" />
<p class="caption">
Figure 9: Transforming shape and color independently: (A) original individual image, (B) shape only, (C), color only, (D) both shape and color.
</p>
</div>
</div>
<div id="symmetrising" class="section level3">
<h3>Symmetrising</h3>
<p>Although a common technique <span class="citation">(e.g., Mealey, Bridgstock, and Townsend 1999)</span>, left-left and right-right mirroring (Fig. <a href="#fig:mirror-sym">10</a>) is not recommended for investigating perceptions of facial symmetry. This is because this method typically produces unnatural images for any face that isn’t already perfectly symmetric. For example, if the nose does not lie in a perfectly straight line from the centre point between the eyes to the centre of the mouth, then one of the mirrored halves will have a much wider nose than the original face, while the the other half will have a much narrower nose than the original face. In extreme cases, one mirrored version can end up with three nostrils and the other with a single nostril.</p>
<div class="figure"><span id="fig:mirror-sym"></span>
<img src="index_files/figure-html/mirror-sym-1.png" alt="Left-left (top) and right-right (bottom) mirrored images. The code for making these images is in the supplemental materials, but we only recommend using this method to demonstrate how misleading it is." width="100%" />
<p class="caption">
Figure 10: Left-left (top) and right-right (bottom) mirrored images. The code for making these images is in the supplemental materials, but we only recommend using this method to demonstrate how misleading it is.
</p>
</div>
<p>A morph-based technique is a more realistic way to manipulate symmetry <span class="citation">Paukner et al. (2017)</span>. It preserves the individual’s characteristic feature shapes and avoids the problem of having to choose an axis of symmetry on a face that isn’t perfectly symmetrical. In this method, the original face is mirror-reversed and each template point is re-labelled. The original and mirrored images are averaged together to create a perfectly symmetric version of the image that has the same feature widths as the original face (Fig. <a href="#fig:morph-sym">11</a>). You can also use this symmetric version to create asymmetric versions of the original face through transforming, exaggerating the differences between the original and the symmetric version.</p>
<pre class="r"><code>sym_both &lt;- symmetrize(f)
sym_shape &lt;- symmetrize(f, color = 0)
sym_color &lt;- symmetrize(f, shape = 0)
sym_anti &lt;- symmetrize(f, shape = -1.0, color = 0)</code></pre>
<div class="figure"><span id="fig:morph-sym"></span>
<img src="index_files/figure-html/morph-sym-1.png" alt="Images with different types of symmetry: (A) symmetric shape and color, (B) symmetric shape, (C) symmetric color, (D) asymmetric shape." width="100%" />
<p class="caption">
Figure 11: Images with different types of symmetry: (A) symmetric shape and color, (B) symmetric shape, (C) symmetric color, (D) asymmetric shape.
</p>
</div>
</div>
</div>
<div id="case-study" class="section level2">
<h2>Case Study</h2>
<div id="london-face-set" class="section level3">
<h3>London Face Set</h3>
<p>We will use the open-source, CC-BY licensed image set, the Face Research Lab London Set <span class="citation">(L. M. DeBruine and Jones 2017b)</span>. Images are of 102 adults whose pictures were taken in London, UK, in April 2012 for a project with Nikon camera (Fig. <a href="#fig:london-set">12</a>). All individuals were paid and gave signed consent for their images to be “used in lab-based and web-based studies in their original or altered forms and to illustrate research (e.g., in scientific journals, news media or presentations).”</p>
<div class="figure"><span id="fig:london-set"></span>
<img src="index_files/figure-html/london-set-1.png" alt="The 102 neutral front faces in the London Face Set." width="100%" />
<p class="caption">
Figure 12: The 102 neutral front faces in the London Face Set.
</p>
</div>
<p>Each subject has one smiling and one neutral pose. For each pose, 5 full colour images were simultaneously taken from different angles: left profile, left three-quarter, front, right three-quarter, and right profile, but we will only use the front-facing images in the examples below. These images were cropped to 1350x1350 pixels and the faces were manually centered (I did this many years ago before I made the tools in this paper). The neutral front images have template files that mark out 189 coordinates delineating face shape for use with Psychomorph or WebMorph.</p>
</div>
<div id="protoypes" class="section level3">
<h3>Protoypes</h3>
<p>The first step for many types of stimuli is to create prototype faces for some categories, such as expression or gender. The faces that make up these averages should be matched for other characteristics that you want to avoid confounding with the categories of interest, such as age or ethnicity. Here, we will choose 5 Black female faces, automatically delineate them, align the images, and create neutral and smiling prototypes (Fig. <a href="#fig:emo-avg">13</a>).</p>
<pre class="r"><code># select the relevant images and auto-delineate them
neu_orig &lt;- subset(london, face_gender == &quot;female&quot;) %&gt;% 
  subset(face_eth == &quot;black&quot;) %&gt;% subset(1:5) %&gt;%
  auto_delin(replace = TRUE)

smi_orig &lt;- subset(smiling, face_gender == &quot;female&quot;) %&gt;% 
  subset(face_eth == &quot;black&quot;) %&gt;% subset(1:5) %&gt;%
  auto_delin(replace = TRUE)

# align the images
aligned &lt;- c(neu_orig, smi_orig) %&gt;%
  align(procrustes = TRUE, patch = TRUE) %&gt;%
  crop(.6, .8, y_off = 0.05)

neu &lt;- subset(aligned, 1:5)
smi &lt;- subset(aligned, 6:10)

neu_avg &lt;- avg(neu, texture = FALSE)
smi_avg &lt;- avg(smi, texture = FALSE)</code></pre>
<div class="figure"><span id="fig:emo-avg"></span>
<img src="index_files/figure-html/emo-avg-1.png" alt="Average and individual neutral and smiling faces." width="100%" />
<p class="caption">
Figure 13: Average and individual neutral and smiling faces.
</p>
</div>
</div>
<div id="emotion-continuum" class="section level3">
<h3>Emotion Continuum</h3>
<p>Once you have two prototype images, you can set up a continuum that morphs between the images and even exaggerates beyond them (Fig. <a href="#fig:continuum">14</a>). Note that some exaggerations beyond the prototypes can produce impossible shape configurations, such as the negative smile, where the open lips from a smile go to closed at 0% and pass through each other at negative values.</p>
<pre class="r"><code>steps &lt;- continuum(neu_avg, smi_avg, from = -0.5, to = 1.5, by = 0.25)</code></pre>
<div class="figure"><span id="fig:continuum"></span>
<img src="index_files/figure-html/continuum-1.png" alt="Continuum from -50% to +150% smiling." width="100%" />
<p class="caption">
Figure 14: Continuum from -50% to +150% smiling.
</p>
</div>
</div>
<div id="sexual-dimorphism-transform" class="section level3">
<h3>Sexual dimorphism transform</h3>
<p>We can use the full templates to create sexual dimorphism transforms from neutral faces. Repeat the process above for 5 male and 5 female neutral faces, skipping the auto-delineation (Fig. <a href="#fig:sexdim-avg">15</a>).</p>
<pre class="r"><code># select the relevant images
f_orig &lt;- subset(london, face_gender == &quot;female&quot;) %&gt;% 
  subset(face_eth == &quot;black&quot;) %&gt;% subset(1:5)

m_orig &lt;- subset(london, face_gender == &quot;male&quot;) %&gt;% 
  subset(face_eth == &quot;black&quot;) %&gt;% subset(1:5)

# align the images
aligned &lt;- c(f_orig, m_orig) %&gt;%
  align(procrustes = TRUE, patch = TRUE) %&gt;%
  crop(.6, .8, y_off = 0.05)

f &lt;- subset(aligned, 1:5)
m &lt;- subset(aligned, 6:10)

f_avg &lt;- avg(f, texture = FALSE)
m_avg &lt;- avg(m, texture = FALSE)</code></pre>
<div class="figure"><span id="fig:sexdim-avg"></span>
<img src="index_files/figure-html/sexdim-avg-1.png" alt="Average and individual female and male faces." width="100%" />
<p class="caption">
Figure 15: Average and individual female and male faces.
</p>
</div>
<p>Next, transform each individual image using the average female and male faces as transform endpoints (Fig. <a href="#fig:sexdim-transform">16</a>).</p>
<pre class="r"><code># use a named vector for shape to automatically rename the images
sexdim &lt;- trans(
  trans_img = c(f, m),
  from_img = f_avg,
  to_img = m_avg,
  shape = c(fem = -.5, masc = .5)
)</code></pre>
<div class="figure"><span id="fig:sexdim-transform"></span>
<img src="index_files/figure-html/sexdim-transform-1.png" alt="Versions of individual faces with (A) 50% feminised shape and (B) 50% masculinized shape." width="100%" />
<p class="caption">
Figure 16: Versions of individual faces with (A) 50% feminised shape and (B) 50% masculinized shape.
</p>
</div>
</div>
<div id="self-resemblance-transform" class="section level3">
<h3>Self-resemblance transform</h3>
<p>Much of my own research involves creating “virtual siblings” for participants to test how they perceive and behave towards strangers with phenotypic kinship cues [<span class="citation">L. M. DeBruine (2004)</span>;DeBruine_2005PRSLB;DeBruine_2011PNAS]. As discussed in detail in DeBruine et al. <span class="citation">(2008)</span>, while morphing techniques are sufficient to create same-gender virtual siblings, transforming techniques are required to make other-gender virtual siblings without confounding self-resemblance with androgyny (Fig. <a href="#fig:virtual-sibs">17</a>).</p>
<pre class="r"><code>virtual_sis &lt;- trans(
  trans_img = f_avg,   # transform an average female face
  shape = 0.5,         # by 50% of the shape differences
  from_img = m_avg,    # between an average male face
  to_img = m) %&gt;%      # and individual male faces
  mask(c(&quot;face&quot;, &quot;neck&quot;,&quot;ears&quot;)) 

virtual_bro &lt;- trans(
  trans_img = m_avg,   # transform an average male face
  shape = 0.5,         # by 50% of the shape differences
  from_img = m_avg,    # between an average male face
  to_img = m) %&gt;%      # and individual male faces
  mask(c(&quot;face&quot;, &quot;neck&quot;,&quot;ears&quot;))</code></pre>
<div class="figure"><span id="fig:virtual-sibs"></span>
<img src="index_files/figure-html/virtual-sibs-1.png" alt="Creating vitual siblings: (A) original images, (B) virtual brothers, (C) virtual sisters." width="100%" />
<p class="caption">
Figure 17: Creating vitual siblings: (A) original images, (B) virtual brothers, (C) virtual sisters.
</p>
</div>
</div>
<div id="labels" class="section level3">
<h3>Labels</h3>
<p>Many social perception studies require labelled images, such a minimal group designs. You can add custom labels and superimpose images on stimuli (Fig. <a href="#fig:label-comp">18</a>).</p>
<pre class="r"><code>flags &lt;- read_stim(&quot;images/flags&quot;)

ingroup &lt;- f %&gt;%
  # pad 10% at the top with matching color
  pad(0.1, 0, 0, 0, patch = TRUE) %&gt;% 
  label(&quot;Scottish&quot;, &quot;north&quot;, &quot;+0+10&quot;) %&gt;%
  image_func(&quot;composite&quot;, flags$saltire$img, 
              gravity = &quot;northeast&quot;, offset = &quot;+10+10&quot;)

outgroup &lt;- f %&gt;%
  pad(0.1, 0, 0, 0, patch = TRUE) %&gt;% 
  label(&quot;Welsh&quot;, &quot;north&quot;, &quot;+0+10&quot;) %&gt;%
  image_func(&quot;composite&quot;, flags$ddraig$img, 
             gravity = &quot;northeast&quot;, offset = &quot;+10+10&quot;)</code></pre>
<div class="figure"><span id="fig:label-comp"></span>
<img src="index_files/figure-html/label-comp-1.png" alt="Stimuli with text labels and superimposed images." width="100%" />
<p class="caption">
Figure 18: Stimuli with text labels and superimposed images.
</p>
</div>
</div>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<ul>
<li>head position in 2D images
<ul>
<li>morphometics</li>
<li>facefuns</li>
</ul></li>
<li>Natural vs standardised source images
<ul>
<li>right image for the question</li>
</ul></li>
<li>Averaging is N=1</li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>We used R [Version 4.0.2; <span class="citation">R Core Team (2020)</span>] and the R-packages <em>dplyr</em> [Version 1.0.5; <span class="citation">Wickham et al. (2021)</span>], <em>ggplot2</em> [Version 3.3.3; <span class="citation">Wickham (2016)</span>], <em>kableExtra</em> [Version 1.3.4; <span class="citation">Zhu (2021)</span>], <em>magick</em> [Version 2.7.1; <span class="citation">Ooms (2021)</span>], <em>papaja</em> [Version 0.1.0.9997; <span class="citation">Aust and Barth (2020)</span>], <em>testthat</em> [Version 3.0.2; <span class="citation">Wickham (2011)</span>], <em>tidyr</em> [Version 1.1.3; <span class="citation">Wickham (2021)</span>], and <em>webmorphR</em> [Version 0.0.1.9005; <span class="citation">L. DeBruine (2021)</span>] to produce this manuscript.</p>
<div id="refs" class="references csl-bib-body hanging-indent" custom-style="Bibliography">
<div id="ref-R-papaja" class="csl-entry">
Aust, Frederik, and Marius Barth. 2020. <em><span class="nocase">papaja</span>: <span>Create</span> <span>APA</span> Manuscripts with <span>R Markdown</span></em>. <a href="https://github.com/crsh/papaja">https://github.com/crsh/papaja</a>.
</div>
<div id="ref-barrgeneralizing" class="csl-entry">
Barr, Dale J. 2007. <span>“Generalizing over Encounters.”</span> In <em>The Oxford Handbook of Psycholinguistics</em>. Oxford University Press, USA.
</div>
<div id="ref-benson1993extracting" class="csl-entry">
Benson, Philip J, and David I Perrett. 1993. <span>“Extracting Prototypical Facial Images from Exemplars.”</span> <em>Perception</em> 22 (3): 257–62.
</div>
<div id="ref-burton2005robust" class="csl-entry">
Burton, A Mike, Rob Jenkins, Peter JB Hancock, and David White. 2005. <span>“Robust Representations for Face Recognition: The Power of Averages.”</span> <em>Cognitive Psychology</em> 51 (3): 256–84.
</div>
<div id="ref-caulfield2016judging" class="csl-entry">
Caulfield, Frances, Louise Ewing, Samantha Bank, and Gillian Rhodes. 2016. <span>“Judging Trustworthiness from Faces: Emotion Cues Modulate Trustworthiness Judgments in Young Children.”</span> <em>British Journal of Psychology</em> 107 (3): 503–18.
</div>
<div id="ref-R-webmorphR" class="csl-entry">
DeBruine, Lisa. 2021. <em>webmorphR: Reproducible Stimuli</em>. Zenodo. <a href="https://doi.org/10.5281/zenodo.???????">https://doi.org/10.5281/zenodo.???????</a>
</div>
<div id="ref-DeBruine_2004PRSLB" class="csl-entry">
DeBruine, Lisa M. 2004. <span>“Facial Resemblance Increases the Attractiveness of Same-Sex Faces More Than Other-Sex Faces.”</span> <em>Proceedings of the Royal Society of London B</em> 271: 2085–90. <a href="https://doi.org/10.1098/rspb.2004.2824">https://doi.org/10.1098/rspb.2004.2824</a>.
</div>
<div id="ref-webmorph" class="csl-entry">
———. 2018. <span>“Debruine/Webmorph: Beta Release 2,”</span> January. <a href="https://doi.org/10.5281/zenodo.1162670">https://doi.org/10.5281/zenodo.1162670</a>.
</div>
<div id="ref-Canada2003" class="csl-entry">
DeBruine, Lisa M., and Benedict C. Jones. 2017a. <span>“Young Adult White Faces with Manipulated Versions.”</span> figshare. <a href="https://doi.org/10.6084/m9.figshare.4220517.v1">https://doi.org/10.6084/m9.figshare.4220517.v1</a>.
</div>
<div id="ref-FRL_London" class="csl-entry">
———. 2017b. <span>“Face Research Lab London Set.”</span> figshare. <a href="https://doi.org/10.6084/m9.figshare.5047666.v5">https://doi.org/10.6084/m9.figshare.5047666.v5</a>.
</div>
<div id="ref-debruine2006correlated" class="csl-entry">
DeBruine, Lisa M., Benedict C. Jones, Anthony C. Little, Lynda G. Boothroyd, David I. Perrett, Ian S. Penton-Voak, Philip A. Cooper, Lars Penke, David R. Feinberg, and Bernard P. Tiddeman. 2006. <span>“Correlated Preferences for Facial Masculinity and Ideal or Actual Partner’s Masculinity.”</span> <em>Proceedings of the Royal Society B: Biological Sciences</em> 273 (1592): 1355–60.
</div>
<div id="ref-DeBruine_2008ASB" class="csl-entry">
DeBruine, Lisa M., Benedict C. Jones, Anthony C. Little, and David I. Perrett. 2008. <span>“Social Perception of Facial Resemblance in Humans.”</span> <em>Archives of Sexual Behavior</em> 37: 64–77. <a href="https://doi.org/10.1007/s10508-007-9266-0">https://doi.org/10.1007/s10508-007-9266-0</a>.
</div>
<div id="ref-ekman1976pictures" class="csl-entry">
Ekman, Paul. 1976. <span>“Pictures of Facial Affect.”</span> <em>Consulting Psychologists Press</em>.
</div>
<div id="ref-Ewing_2010" class="csl-entry">
Ewing, Louise, Gillian Rhodes, and Elizabeth Pellicano. 2010. <span>“Have You Got the Look? Gaze Direction Affects Judgements of Facial Attractiveness.”</span> <em>Visual Cognition</em> 18 (3): 321–30.
</div>
<div id="ref-Fox_2007" class="csl-entry">
Fox, Christopher J, and Jason JS Barton. 2007. <span>“What Is Adapted in Face Adaptation? The Neural Representations of Expression in the Human Visual System.”</span> <em>Brain Research</em> 1127: 80–89.
</div>
<div id="ref-gonzalez2002digital" class="csl-entry">
Gonzalez, Rafael C, Richard E Woods, and others. 2002. <span>“Digital Image Processing.”</span> Prentice hall Upper Saddle River, NJ.
</div>
<div id="ref-Gronenschild_2009" class="csl-entry">
Gronenschild, Ed H. B. M., Floortje Smeets, Eric F. P. M. Vuurman, Martin P. J. van Boxtel, and Jelle Jolles. 2009. <span>“The Use of Faces as Stimuli in Neuroimaging and Psychological Experiments: A Procedure to Standardize Stimulus Features.”</span> <em>Behavior Research Methods</em> 41: 1053–60. <a href="https://doi.org/10.3758/BRM.41.4.1053">https://doi.org/10.3758/BRM.41.4.1053</a>.
</div>
<div id="ref-higham2016matlab" class="csl-entry">
Higham, Desmond J, and Nicholas J Higham. 2016. <em>MATLAB Guide</em>. Vol. 150. Siam.
</div>
<div id="ref-Holtzman_2011" class="csl-entry">
Holtzman, Nicholas S. 2011. <span>“Facing a Psychopath: Detecting the Dark Triad from Emotionally-Neutral Faces, Using Prototypes from the Personality Faceaurus.”</span> <em>Journal of Research in Personality</em> 45 (6): 648–54.
</div>
<div id="ref-holzleitner2019comparing" class="csl-entry">
Holzleitner, Iris J., Anthony J. Lee, Amanda C. Hahn, Michal Kandrik, Jeanne Bovet, Julien P. Renoult, David Simmons, Oliver Garrod, Lisa M. DeBruine, and Benedict C. Jones. 2019. <span>“Comparing Theory-Driven and Data-Driven Attractiveness Models Using Images of Real Women’s Faces.”</span> <em>Journal of Experimental Psychology: Human Perception and Performance</em> 45 (12): 1589.
</div>
<div id="ref-ishai2005face" class="csl-entry">
Ishai, Alumit, Conny F Schmidt, and Peter Boesiger. 2005. <span>“Face Perception Is Mediated by a Distributed Cortical Network.”</span> <em>Brain Research Bulletin</em> 67 (1-2): 87–93.
</div>
<div id="ref-jones2020facial" class="csl-entry">
Jones, Alex L., Christoph Schild, and Benedict C. Jones. 2020. <span>“Facial Metrics Generated from Manually and Automatically Placed Image Landmarks Are Highly Correlated.”</span> <em>Evolution and Human Behavior</em>.
</div>
<div id="ref-jones2019biological" class="csl-entry">
Jones, Alex L, and Bastian Jaeger. 2019. <span>“Biological Bases of Beauty Revisited: The Effect of Symmetry, Averageness, and Sexual Dimorphism on Female Facial Attractiveness.”</span> <em>Symmetry</em> 11 (2): 279.
</div>
<div id="ref-jones2018no" class="csl-entry">
Jones, Benedict C, Amanda C Hahn, Claire I Fisher, Hongyi Wang, Michal Kandrik, Junpeng Lao, Chengyang Han, Anthony J Lee, Iris J Holzleitner, and Lisa M. DeBruine. 2018. <span>“No Compelling Evidence That More Physically Attractive Young Adult Women Have Higher Estradiol or Progesterone.”</span> <em>Psychoneuroendocrinology</em> 98: 1–5.
</div>
<div id="ref-lefevre2013telling" class="csl-entry">
Lefevre, Carmen E, Gary J Lewis, David I Perrett, and Lars Penke. 2013. <span>“Telling Facial Metrics: Facial Width Is Associated with Testosterone Levels in Men.”</span> <em>Evolution and Human Behavior</em> 34 (4): 273–79.
</div>
<div id="ref-little2001self" class="csl-entry">
Little, Anthony C., D. Michael Burt, Ian S. Penton-Voak, and David I. Perrett. 2001. <span>“Self-Perceived Attractiveness Influences Human Female Preferences for Sexual Dimorphism and Symmetry in Male Faces.”</span> <em>Proceedings of the Royal Society of London. Series B: Biological Sciences</em> 268 (1462): 39–44.
</div>
<div id="ref-Little_2011" class="csl-entry">
Little, Anthony C., Benedict C. Jones, and Lisa M. DeBruine. 2011. <span>“Facial Attractiveness: Evolutionary Based Research.”</span> <em>Philosophical Transactions of the Royal Society B</em> 366: 1638–59. <a href="https://doi.org/10.1098/rstb.2010.0404">https://doi.org/10.1098/rstb.2010.0404</a>.
</div>
<div id="ref-Mealey_1999" class="csl-entry">
Mealey, Linda, Ruth Bridgstock, and Grant C Townsend. 1999. <span>“Symmetry and Perceived Facial Attractiveness: A Monozygotic Co-Twin Comparison.”</span> <em>Journal of Personality and Social Psychology</em> 76 (1): 151.
</div>
<div id="ref-Morrison_2018" class="csl-entry">
Morrison, Danielle, Hongyi Wang, Amanda C. Hahn, Benedict C. Jones, and Lisa M. DeBruine. 2018. <span>“Predicting the Reward Value of Faces and Bodies from Social Perceptions: Supplemental Materials.”</span> OSF. <a href="https://doi.org/10.17605/OSF.IO/G27WF">https://doi.org/10.17605/OSF.IO/G27WF</a>.
</div>
<div id="ref-neiworth2007face" class="csl-entry">
Neiworth, Julie J, Janice M Hassett, and Cara J Sylvester. 2007. <span>“Face Processing in Humans and New World Monkeys: The Influence of Experiential and Ecological Factors.”</span> <em>Animal Cognition</em> 10 (2): 125.
</div>
<div id="ref-nishimura2000graphicconverter" class="csl-entry">
Nishimura, Darryl. 2000. <span>“GraphicConverter 3.9. 1.”</span> <em>Biotech Software &amp; Internet Report: The Computer Software Journal for Scient</em> 1 (6): 267–69.
</div>
<div id="ref-R-magick" class="csl-entry">
Ooms, Jeroen. 2021. <em>Magick: Advanced Graphics and Image-Processing in r</em>. <a href="https://CRAN.R-project.org/package=magick">https://CRAN.R-project.org/package=magick</a>.
</div>
<div id="ref-paluszek2019pattern" class="csl-entry">
Paluszek, Michael, and Stephanie Thomas. 2019. <span>“Pattern Recognition with Deep Learning.”</span> In <em>MATLAB Machine Learning Recipes</em>, 209–30. Springer.
</div>
<div id="ref-pantelis2008some" class="csl-entry">
Pantelis, Peter C, Marieke K Van Vugt, Robert Sekuler, Hugh R Wilson, and Michael J Kahana. 2008. <span>“Why Are Some People’s Names Easier to Learn Than Others? The Effects of Face Similarity on Memory for Face-Name Associations.”</span> <em>Memory &amp; Cognition</em> 36 (6): 1182–95.
</div>
<div id="ref-paukner2017capuchin" class="csl-entry">
Paukner, Annika, Lauren J Wooddell, Carmen E Lefevre, Eric Lonsdorf, and Elizabeth Lonsdorf. 2017. <span>“Do Capuchin Monkeys (Sapajus Apella) Prefer Symmetrical Face Shapes?”</span> <em>Journal of Comparative Psychology</em> 131 (1): 73.
</div>
<div id="ref-R-base" class="csl-entry">
R Core Team. 2020. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-Rhodes_2003" class="csl-entry">
Rhodes, Gillian, Linda Jeffery, Tamara L Watson, Colin W G Clifford, and Ken Nakayama. 2003. <span>“Fitting the Mind to the World: Face Adaptation and Attractiveness Aftereffects.”</span> <em>Psychological Science</em> 14 (6): 558–66.
</div>
<div id="ref-rowland1995manipulating" class="csl-entry">
Rowland, Duncan A., and David I. Perrett. 1995. <span>“Manipulating Facial Appearance Through Shape and Color.”</span> <em>IEEE Computer Graphics and Applications</em> 15 (5): 70–76.
</div>
<div id="ref-Scheib_1999" class="csl-entry">
Scheib, Joanna E, Steven W Gangestad, and Randy Thornhill. 1999. <span>“Facial Attractiveness, Symmetry and Cues of Good Genes.”</span> <em>Proceedings of the Royal Society of London. Series B: Biological Sciences</em> 266 (1431): 1913–17.
</div>
<div id="ref-sforza2010my" class="csl-entry">
Sforza, Anna, Ilaria Bufalari, Patrick Haggard, and Salvatore M Aglioti. 2010. <span>“My Face in Yours: Visuo-Tactile Facial Stimulation Influences Sense of Identity.”</span> <em>Social Neuroscience</em> 5 (2): 148–62.
</div>
<div id="ref-CFD_2015" class="csl-entry">
<span>“The Chicago Face Database: A Free Stimulus Set of Faces and Norming Data.”</span> 2015. <em>Behavior Research Methods</em> 47: 1122–35. <a href="https://doi.org/10.3758/s13428-014-0532-5">https://doi.org/10.3758/s13428-014-0532-5</a>.
</div>
<div id="ref-imagemagick" class="csl-entry">
The ImageMagick Development Team. 2021. <em>ImageMagick</em> (version 7.0.10). <a href="https://imagemagick.org">https://imagemagick.org</a>.
</div>
<div id="ref-tiddeman2001prototyping" class="csl-entry">
Tiddeman, Bernard P., D. Michael Burt, and David I. Perrett. 2001. <span>“Prototyping and Transforming Facial Textures for Perception Research.”</span> <em>IEEE Computer Graphics and Applications</em> 21 (5): 42–50.
</div>
<div id="ref-tiddeman2005towards" class="csl-entry">
Tiddeman, Bernard P., Michael R. Stirrat, and David I. Perrett. 2005. <span>“Towards Realism in Facial Image Transformation: Results of a Wavelet MRF Method.”</span> In <em>Computer Graphics Forum</em>, 24:449–56. 3. Blackwell Publishing, Inc Oxford, UK; Boston, USA.
</div>
<div id="ref-tvrebicky2016focal" class="csl-entry">
Třebickỳ, Vı́t, Jitka Fialová, Karel Kleisner, and Jan Havlı́ček. 2016. <span>“Focal Length Affects Depicted Shape and Perception of Facial Images.”</span> <em>PLoS One</em> 11 (2): e0149313.
</div>
<div id="ref-visconti2014facilitated" class="csl-entry">
Visconti di Oleggio Castello, Matteo, J Swaroop Guntupalli, Hua Yang, and M Ida Gobbini. 2014. <span>“Facilitated Detection of Social Cues Conveyed by Familiar Faces.”</span> <em>Frontiers in Human Neuroscience</em> 8: 678.
</div>
<div id="ref-Weigelt_2013" class="csl-entry">
Weigelt, Sarah, Kami Koldewyn, and Nancy Kanwisher. 2013. <span>“Face Recognition Deficits in Autism Spectrum Disorders Are Both Domain Specific and Process Specific.”</span> <em>PloS One</em> 8 (9): e74541.
</div>
<div id="ref-R-testthat" class="csl-entry">
Wickham, Hadley. 2011. <span>“Testthat: Get Started with Testing.”</span> <em>The R Journal</em> 3: 5–10. <a href="https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf">https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf</a>.
</div>
<div id="ref-R-ggplot2" class="csl-entry">
———. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>.
</div>
<div id="ref-R-tidyr" class="csl-entry">
———. 2021. <em>Tidyr: Tidy Messy Data</em>. <a href="https://CRAN.R-project.org/package=tidyr">https://CRAN.R-project.org/package=tidyr</a>.
</div>
<div id="ref-R-dplyr" class="csl-entry">
Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. <em>Dplyr: A Grammar of Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr">https://CRAN.R-project.org/package=dplyr</a>.
</div>
<div id="ref-R-kableExtra" class="csl-entry">
Zhu, Hao. 2021. <em>kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax</em>. <a href="https://CRAN.R-project.org/package=kableExtra">https://CRAN.R-project.org/package=kableExtra</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
